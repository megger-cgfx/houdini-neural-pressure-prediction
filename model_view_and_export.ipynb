{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4TxcjabrWG4g",
    "outputId": "ec84f0e2-80c6-4d06-dbe2-7dc25c152224"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsOqrTE0m7V8"
   },
   "source": [
    "---\n",
    "# Ingest Data, Visualize and save to drive\n",
    "\n",
    "Functions to load data and visualize fields\n",
    "\n",
    "Saving to drive as single file to speed up load times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cWXHEmWqOPGq",
    "outputId": "acec7072-b5ef-4b7f-fd24-e11d31b222b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr8W2eFLtyX2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_fields(fields, value_range=0.2):\n",
    "    f = plt.figure( figsize=(20,12) )\n",
    "    for idx, field in enumerate(fields):\n",
    "        # Debug, plot figure\n",
    "        f.add_subplot(1, 3, idx + 1)\n",
    "        plt.imshow(field, vmin=-value_range, vmax=value_range)\n",
    "    plt.show(block=True)\n",
    "    \n",
    "def show_fields_diff(inputfile1, inputfile2, value_scale=1/20):\n",
    "    with np.load(inputfile1) as data:\n",
    "        velx = data['velx'][:,:,0] * value_scale\n",
    "        vely = data['vely'][:,:,0] * value_scale\n",
    "        pres = data['pres'][:,:,0] * value_scale\n",
    "        print(\"shape: \", velx.shape, vely.shape, pres.shape)\n",
    "    with np.load(inputfile2) as data:\n",
    "        velx2 = data['velx'][:,:,0] * value_scale\n",
    "        vely2 = data['vely'][:,:,0] * value_scale\n",
    "        pres2 = data['pres'][:,:,0] * value_scale\n",
    "        print(\"shape: \", velx2.shape, vely2.shape, pres2.shape)\n",
    "    \n",
    "    fields1 = [velx, vely, pres]\n",
    "    fields2 = [velx2, vely2, pres2]\n",
    "    fields_diff = [velx-velx2, vely-vely2, pres-pres2]\n",
    "        \n",
    "    plot_fields(fields1)\n",
    "    plot_fields(fields2)\n",
    "    plot_fields(fields_diff)\n",
    "    \n",
    "\n",
    "testfile = (\"./data2D/hou_data/v4/npz_train/pyro1.w5.1200.pre_project.npz\")\n",
    "testfile2 = (\"./data2D/hou_data/v4/npz_train/pyro1.w5.1200.post_project.npz\")\n",
    "show_fields_diff(testfile, testfile2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKrWw8rrJOEk"
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# LOAD DATA 2D -----------------\n",
    "# ------------------------------\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "X_train_glob = sorted(glob.glob(\"./data2D/npz_train/pyro_bill*pre*\"))\n",
    "Y_train_glob = sorted(glob.glob(\"./data2D/npz_train/pyro_bill*post*\"))\n",
    "X_test_glob  = sorted(glob.glob(\"./data2D/npz_test/pyro_bill*pre*\"))\n",
    "Y_test_glob  = sorted(glob.glob(\"./data2D/npz_test/pyro_bill*post*\"))\n",
    "\n",
    "# used for scaling the data\n",
    "scaler = 1/20\n",
    "\n",
    "try:\n",
    "    #find base shape:\n",
    "    with np.load(X_train_glob[20]) as data:\n",
    "        pres = data['pres'][:,:,0]\n",
    "        target_shape = pres.shape\n",
    "    print(\"TARGET SHAPE: \", target_shape)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "def load_sim_files(filelist, verbose=0):\n",
    "    dir = os.path.dirname(filelist[0])\n",
    "    perf_start = time.perf_counter()\n",
    "    array = []\n",
    "    total_files = len(filelist)\n",
    "    \n",
    "    print(f\"\\nStarting to load {total_files} files from {dir}.\")\n",
    "    for idx, file in enumerate(filelist):\n",
    "        perf0 = time.perf_counter()  # PERF\n",
    "\n",
    "        try:\n",
    "            with np.load(file) as data:\n",
    "                velx = data['velx'][:,:,0]\n",
    "                vely = data['vely'][:,:,0]\n",
    "                pres = data['pres'][:,:,0]\n",
    "        except Exception:\n",
    "            bad_filename = os.path.split(file)[1]\n",
    "            bad_frame = bad_filename.split(\".\")[-3]\n",
    "            bad_name = bad_filename.split(\".\")[0]\n",
    "            source_folder = os.path.dirname(file)\n",
    "            kill_filename = f\"{bad_name}*{bad_frame}*\"\n",
    "            kill_selection = glob.glob(os.path.join(source_folder,kill_filename))\n",
    "            print(\"kill_selection: \", kill_selection)\n",
    "\n",
    "        if verbose:  # PERF\n",
    "            print(f\"Loading file Nr. {idx} from {total_files}\")\n",
    "            perf1 = time.perf_counter()\n",
    "            print(f\"\\tP1 loaded data after: {(perf1 - perf0)*1000:.1f}ms\"),\n",
    "\n",
    "        velx = np.pad(velx, ((0,1),(0,1)))\n",
    "        vely = np.pad(vely, ((0,1),(0,1)))\n",
    "\n",
    "        #print(\"Minmax (Velx, Vely, Pres): \\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t\\t{}\".format(round(velx.min(),2), round(velx.max(),2), round(vely.min(),2), round(vely.max(),2), round(pres.min(),2), round(pres.max(),2), file))\n",
    "        (round(velx.min(),2), round(velx.max(),2), round(vely.min(),2), round(vely.max(),2), round(pres.min(),2), round(pres.max(),2), file)\n",
    "        #print(shapes)\n",
    "        \n",
    "        if verbose:  # PERF\n",
    "            perf2 = time.perf_counter()\n",
    "            print(f\"\\tP2 displayed min/max after: {(perf2 - perf1)*1000:.1f}ms\")\n",
    "\n",
    "        fields = np.stack([velx, vely, pres])\n",
    "        fields = np.moveaxis(fields, 0, 2)\n",
    "        fields = fields * scaler\n",
    "        \n",
    "        if verbose:  # PERF\n",
    "            perf3 = time.perf_counter()\n",
    "            print(f\"\\tP3 transformed data after: {(perf3 - perf2)*1000:.1f}ms\")\n",
    "\n",
    "        array.append(fields)\n",
    "\n",
    "        if verbose:  # PERF\n",
    "            perf4 = time.perf_counter()\n",
    "            print(f\"\\tP4 appended array after: {(perf4 - perf3)*1000:.1f}ms\")\n",
    "\n",
    "    print(f\"\\n\\t\\tLoaded {total_files} files from: {dir}\")\n",
    "    print(f\"\\t\\tTotal time taken: {time.perf_counter() - perf_start:.1f} seconds. \\n\")\n",
    "    return array\n",
    "\n",
    "if(len(X_train_glob)>0):\n",
    "    X_train = load_sim_files(X_train_glob)\n",
    "    Y_train = load_sim_files(Y_train_glob)\n",
    "\n",
    "    X_test = load_sim_files(X_test_glob)\n",
    "    Y_test = load_sim_files(Y_test_glob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "id": "UlloSIx6jIZQ",
    "outputId": "9ced67cd-25eb-4c5f-8ffd-b08b2978c284"
   },
   "outputs": [],
   "source": [
    "# # SAVE SPARSELY LOADED DATA TO SINGLE FILE:\n",
    "# try:\n",
    "#     print(X_train.shape)\n",
    "#     np.savez_compressed(\"./data2D/loaded_data.npz\",\n",
    "#                         X_train = X_train,\n",
    "#                         Y_train = Y_train,\n",
    "#                         X_test = X_test,\n",
    "#                         Y_test = Y_test)\n",
    "# except:  \n",
    "#     # if not defined, don't overwite files on disk\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2G821TZMnMK4"
   },
   "source": [
    "---\n",
    "# Load compiled data from disk\n",
    "(faster loading)\n",
    "\n",
    "Also calculating Y vel delta from Y(vel)-X(vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2Ij3gaOL9v4",
    "outputId": "2e6dba38-ace2-4b3b-be37-157795fad940"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "    %cd \"/content/drive/MyDrive/ML_RnD_pressurepredict\"\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0qJG-BLjKqH"
   },
   "outputs": [],
   "source": [
    "combined_data_path = \"./data2D/loaded_data.npz\"\n",
    "with np.load(combined_data_path) as data:\n",
    "    X_train = data[\"X_train\"]\n",
    "    Y_train = data[\"Y_train\"]\n",
    "    X_test  = data[\"X_test\"]\n",
    "    Y_test  = data[\"Y_test\"]\n",
    "\n",
    "Y_train_orig = Y_train.copy()\n",
    "Y_test_orig  = Y_test.copy()\n",
    "\n",
    "##### Change Y to vel_diff:\n",
    "# Vel X\n",
    "# Y_train[:,:,:,0] = Y_train_orig[:,:,:,0] - X_train[:,:,:,0]\n",
    "for idx, Y in enumerate(Y_train_orig):\n",
    "    Y_train[idx][:,:,0] = Y_train_orig[idx][:,:,0] - X_train[idx][:,:,0]  # X dim\n",
    "    Y_train[idx][:,:,1] = Y_train_orig[idx][:,:,1] - X_train[idx][:,:,1]  # Y dim\n",
    "for idx, Y in enumerate(Y_test_orig):\n",
    "    Y_test[idx][:,:,0] = Y_test_orig[idx][:,:,0] - X_test[idx][:,:,0]  # X dim\n",
    "    Y_test[idx][:,:,1] = Y_test_orig[idx][:,:,1] - X_test[idx][:,:,1]  # Y dim\n",
    "\n",
    "del Y_train_orig\n",
    "del Y_test_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9WSavl_Nru3"
   },
   "outputs": [],
   "source": [
    "print(\"Loaded X shape: \", np.array(X_train).shape, np.array(X_test).shape)\n",
    "print(\"Loaded Y shape: \", np.array(Y_train).shape, np.array(Y_test).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52mMH3QTbn3p"
   },
   "outputs": [],
   "source": [
    "# Resize inputs to random slices and flip randomly\n",
    "def randomize_crop_data_2d(input_data, n, seed=42):\n",
    "    \"\"\"Function to transform input to random slices of 64 voxels (in 2D)\n",
    "\n",
    "    Args:\n",
    "        input_data (List of np.array): list of training data\n",
    "        n (int): number of variations, looping through input\n",
    "\n",
    "    \"\"\"\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    input_shape = np.array(input_data).shape\n",
    "    \n",
    "    output = []\n",
    "    for i in range(n):\n",
    "        index = i % input_shape[0]\n",
    "        source_sample = input_data[index]\n",
    "\n",
    "        # Randomly crop to desired size using TensorFlow\n",
    "        random_sample = tf.image.random_crop(source_sample, size=(64, 64, 3))\n",
    "\n",
    "        # Randomly flip along either x or y\n",
    "        flip_rng = np.random.random()\n",
    "        if flip_rng < 1/3:\n",
    "            if flip_rng < 2/3: axis = 0\n",
    "            else: axis = 1\n",
    "            random_sample = tf.image.flip_left_right(random_sample) if axis == 1 else tf.image.flip_up_down(random_sample)\n",
    "\n",
    "        # Randomly rotate by 90 degrees\n",
    "        rot_rng = int(np.random.random() * 4)\n",
    "        random_sample = tf.image.rot90(random_sample, k=rot_rng)\n",
    "\n",
    "        # Convert the tensor to a NumPy array before appending to the output list\n",
    "        output.append(random_sample.numpy())\n",
    "\n",
    "    return output\n",
    "\n",
    "print(\"X_train:     \", X_train.shape)\n",
    "print(\"X_test:      \", X_test.shape)\n",
    "\n",
    "ntrain = 1500\n",
    "ntest = round(ntrain/5)\n",
    "X_train_rand = randomize_crop_data_2d(X_train, ntrain, seed=42)\n",
    "Y_train_rand = randomize_crop_data_2d(Y_train, ntrain, seed=42)\n",
    "X_test_rand  = randomize_crop_data_2d(X_test,  ntest,  seed=100)\n",
    "Y_test_rand  = randomize_crop_data_2d(Y_test,  ntest,  seed=100)\n",
    "\n",
    "print(\"X_train_rand: \", np.array(X_train_rand).shape)\n",
    "print(\"X_test_rand:  \", np.array(X_test_rand).shape)\n",
    "\n",
    "# 24-07-06 # BEWARE OF RANDOM SAMPLE FLIP AND ROTATE NOT WORKING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOwDANkMWU3w"
   },
   "source": [
    "## Visualize Loaded Data\n",
    "Especially to sanity-check coherence of variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFSCbXHAJ-d7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_fields_array(array, value_scale=0.15, size=(20,12)):\n",
    "    '''Visualize a 3d Array of [VelX, VelY, Pres]'''\n",
    "    velx = array[:,:,0]\n",
    "    vely = array[:,:,1]\n",
    "    pres = array[:,:,2]\n",
    "    #print(\"shape: \", velx.shape, vely.shape, pres.shape)\n",
    "    fields = [velx, vely, pres]\n",
    "    titles = [\"vel.x\", \"vel.y\", \"pressure\"]\n",
    "    f = plt.figure( figsize=size )\n",
    "    for idx, field in enumerate(fields):\n",
    "        # Debug, plot figure\n",
    "        f.add_subplot(1, 3, idx + 1)\n",
    "        plt.title(titles[idx], loc='left')\n",
    "        plt.imshow(field, vmin=-value_scale, vmax=value_scale)\n",
    "    plt.show(block=True)\n",
    "    print(\"Minmax (Velx, Vely, Pres): \\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(round(velx.min(),2), round(velx.max(),2), round(vely.min(),2), round(vely.max(),2), round(pres.min(),2), round(pres.max(),2)))\n",
    "\n",
    "# Visualize first instance of train test split\n",
    "prev_idx = 10\n",
    "print(f\"\\nTrain X and Y at idx {prev_idx}: \")\n",
    "show_fields_array(X_train[prev_idx], value_scale=.15)\n",
    "show_fields_array(Y_train[prev_idx], value_scale=.05)\n",
    "\n",
    "for i in range(4):\n",
    "    var = len(X_train)*i + prev_idx\n",
    "    print(f\"\\n\\nRandom Variation Nr. {i+1} of same \")\n",
    "    show_fields_array(X_train_rand[var], value_scale=.25, size=(15,8))\n",
    "    show_fields_array(Y_train_rand[var], value_scale=.1, size=(15,8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLeUlyVbAI0b"
   },
   "source": [
    "## Build Datasets\n",
    "from tf.data.Dataset.from_tensor_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0OjEDELAH6e"
   },
   "outputs": [],
   "source": [
    "# APPLY BATCHING (with 1)\n",
    "# ... and achieve output dimensionality of:     [2,     n, 64, 64, 3] \n",
    "# ... with desired iteration dimensionality of:     [None, 64, 64, 3]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices( (np.array(X_train_rand), np.array(Y_train_rand)) )\n",
    "train_dataset = train_dataset.shuffle(1500)\n",
    "train_dataset = train_dataset.batch(5)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices( (np.array(X_test_rand), np.array(Y_test_rand)) )\n",
    "test_dataset = test_dataset.shuffle(300)\n",
    "test_dataset = test_dataset.batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDedwAvUFUF1"
   },
   "outputs": [],
   "source": [
    "print(\"Sanity check iterable dimension: \\n\\t\", tf.shape(next(iter(test_dataset))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2bbwoc2mi9z"
   },
   "source": [
    "---\n",
    "# Build Generator\n",
    "Referenced from pix2pix TF tutorial (https://www.tensorflow.org/tutorials/generative/pix2pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqjNHb2yalhD"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Determine input and coding size from\n",
    "input_shape = X_train_rand[0].shape\n",
    "# Define output dimensionality\n",
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "print(\"Input size: \", input_shape)\n",
    "\n",
    "#tf.keras.mixed_precision.set_global_policy('float32')\n",
    "# Set dtype to mixed precision for better perf.: Sometimes calculate in float 16\n",
    "# Disabled for testing without GPU\n",
    "#tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZDhBPfBquVP"
   },
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                                kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())    \n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def resnet_block(input_tensor, filters, kernel_size=3):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')(input_tensor)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Add the skip connection (input + output)\n",
    "    x = tf.keras.layers.Add()([x, input_tensor])\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def Generator():\n",
    "    inputs = tf.keras.layers.Input(shape= input_shape )\n",
    "    \n",
    "    size = 3\n",
    "\n",
    "    down_stack = [\n",
    "        downsample(32,  size, apply_batchnorm=False),\n",
    "        downsample(64,  size),\n",
    "        downsample(128, size),\n",
    "        downsample(256, size),\n",
    "    ]\n",
    "    \n",
    "    num_res_blocks = 2  # This is an example; you can change it\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(256, size, apply_dropout=True),\n",
    "        upsample(128, size),\n",
    "        upsample(64,  size),\n",
    "        upsample(32,  size),\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                           strides=2,\n",
    "                                           padding='same',\n",
    "                                           kernel_initializer=initializer,\n",
    "                                           activation='tanh')\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "        \n",
    "    # Adding ResNet blocks after downsampling\n",
    "    for _ in range(num_res_blocks):\n",
    "        x = resnet_block(x, filters=256)  # Adjust the filters as needed\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZDhBPfBquVP"
   },
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "        tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                                kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())    \n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    if apply_dropout:\n",
    "        result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "def resnet_block(input_tensor, filters, kernel_size=3):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')(input_tensor)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Add the skip connection (input + output)\n",
    "    x = tf.keras.layers.Add()([x, input_tensor])\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def Generator():\n",
    "    inputs = tf.keras.layers.Input(shape= input_shape )\n",
    "    \n",
    "    size = 3\n",
    "\n",
    "    down_stack = [\n",
    "        downsample(32,  size, apply_batchnorm=False),\n",
    "        downsample(64,  size),\n",
    "        downsample(128, size),\n",
    "        downsample(256, size),\n",
    "    ]\n",
    "    \n",
    "    num_res_blocks = 2  # This is an example; you can change it\n",
    "\n",
    "    up_stack = [\n",
    "        upsample(256, size, apply_dropout=True),\n",
    "        upsample(128, size),\n",
    "        upsample(64,  size),\n",
    "        upsample(32,  size),\n",
    "    ]\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                           strides=2,\n",
    "                                           padding='same',\n",
    "                                           kernel_initializer=initializer,\n",
    "                                           activation='tanh')\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    # Downsampling through the model\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "        \n",
    "    # Adding ResNet blocks after downsampling\n",
    "    for _ in range(num_res_blocks):\n",
    "        x = resnet_block(x, filters=256)  # Adjust the filters as needed\n",
    "\n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    # Upsampling and establishing the skip connections\n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "    x = last(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpQjMX94qxxT"
   },
   "outputs": [],
   "source": [
    "# test down and upsampling\n",
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(tf.expand_dims(X_train_rand[0], 0))\n",
    "print (down_result.shape)\n",
    "\n",
    "up_model = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJP1gQotqzkz"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Opa70auCcSRt"
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(generator, show_shapes=True, show_dtype=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMPjG6nT82Ml"
   },
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# DEFINE GENERATOR LOSS\n",
    "# ---------------------\n",
    "\n",
    "LAMBDA = 100\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # Mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(\n",
    "        tf.cast(target,     tf.float32) - \n",
    "                gen_output\n",
    "    ))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brwsmd3hA97c"
   },
   "outputs": [],
   "source": [
    "# Test the generator\n",
    "value_range = 0.05\n",
    "gen_output = generator(100*X_train_rand[24][tf.newaxis, ...], training=False)\n",
    "plt.imshow(gen_output[0, ...], vmin=-value_range, vmax=value_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUMEKFmM7FBL"
   },
   "source": [
    "---\n",
    "\n",
    "# Build Discriminator\n",
    "Referenced from pix2pix TF tutorial (https://www.tensorflow.org/tutorials/generative/pix2pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKA9jdQP67Xw"
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=input_shape, name='input_image')\n",
    "    tar = tf.keras.layers.Input(shape=input_shape, name='target_image')\n",
    "\n",
    "    x = tf.keras.layers.concatenate([inp, tar])\n",
    "    \n",
    "    down1 = downsample(32, 4, False)(x)\n",
    "    down2 = downsample(64, 4)(down1)\n",
    "    down3 = downsample(128, 4)(down2)\n",
    "\n",
    "    zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)\n",
    "    conv = tf.keras.layers.Conv2D(256, 4, strides=1,\n",
    "                                  kernel_initializer=initializer,\n",
    "                                  use_bias=False)(zero_pad1)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                  kernel_initializer=initializer)(zero_pad2)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKF5-0NEDdqc"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kb9okqUX7o9B"
   },
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzkWUkG1AXea"
   },
   "outputs": [],
   "source": [
    "# Test the disriminator\n",
    "disc_out = discriminator([X_train_rand[0][tf.newaxis, ...], gen_output], training=False)\n",
    "plt.imshow(disc_out[0, ..., -1], vmin=-value_range, vmax=value_range, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKVjJyj7CHDf"
   },
   "outputs": [],
   "source": [
    "# Define discriminator loss\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GpHdDYcCKwE"
   },
   "outputs": [],
   "source": [
    "# Define optimizers and a checkpoint-saver\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2.5e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(9e-5, beta_1=0.5)\n",
    "\n",
    "# TWEAK NOTES\n",
    "# Tweaked overpowering discriminator by slowing learning rate from 2e-4 to 1e-4\n",
    "# and speeding up generator to 2.5e-4 from 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idA2xeF-CZSM"
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    value_range = .05\n",
    "\n",
    "    display_list = [\n",
    "        test_input[0][:,:,0], tar[0][:,:,0], prediction[0][:,:,0],\n",
    "        test_input[0][:,:,1], tar[0][:,:,1], prediction[0][:,:,1],\n",
    "        test_input[0][:,:,2], tar[0][:,:,2], prediction[0][:,:,2] \n",
    "    ]\n",
    "    maintitle = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "    title = ['vel.x', 'vel.y', 'pressure']\n",
    "\n",
    "    for i in range(9):\n",
    "        plt_idx = i\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        \n",
    "        try: header = f\"----- {maintitle[i]} -----\"\n",
    "        except: header = \"\"\n",
    "        plt.title(f\"{header}\\n\\n{title[i//3]}\")\n",
    "        \n",
    "        # Getting the pixel values in the [0, 1] range to plot.\n",
    "        a, b = 0.5, 0.1\n",
    "        tile = display_list[i]\n",
    "        if (i-2)%3 == 0:\n",
    "            # Blur prediction with gaussian sigma\n",
    "            BLUR_SIGMA = 0.35\n",
    "            tile = scipy.ndimage.gaussian_filter(tile, BLUR_SIGMA)\n",
    "        plt.imshow(tile, vmin=-value_range, vmax=value_range)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-j36eIvCjJ9"
   },
   "outputs": [],
   "source": [
    "prev_idx = 10\n",
    "generate_images(generator, X_train_rand[prev_idx][tf.newaxis, ...], Y_train_rand[prev_idx][tf.newaxis, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pf4VxJwmMSjO"
   },
   "source": [
    "---\n",
    "# Training\n",
    "- For each example input generate an output\n",
    "- Discriminator receives (input image + generated output) and (input image + target image)\n",
    "- Calculate generator and discriminator loss\n",
    "- Calculate gradients of loss and apply to optimizer\n",
    "- Log losses to TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCVFW6OvM057"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "from IPython import display\n",
    "log_dir=\"logs/\"\n",
    "\n",
    "# summary_writer = tf.summary.create_file_writer(\n",
    "#   log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0jKfoPfCWIF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "############# Define checkpoint saver and directory\n",
    "\n",
    "import os\n",
    "checkpoint_dir = './training_checkpointsC'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfMUbiiwNTpl"
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPXvHWF6UeN6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "# fit(train_dataset, test_dataset, steps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHMkaZUT_EVg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TEMP DISABLE\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "# generator.save(\".\\models\\pressure_predict_tf2_Cv4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preview trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "# generator = tf.keras.models.load_model(\".\\models\\pressure_predict_tf2_Cv2.h5\")\n",
    "# print(generator.optimizer)\n",
    "Y_train_rand[40].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prev_idx = 40\n",
    "generator = tf.keras.models.load_model(\".\\models\\pressure_predict_tf2_Cv4.h5\")\n",
    "generate_images(generator, X_train_rand[prev_idx][tf.newaxis, ...], Y_train_rand[prev_idx][tf.newaxis, ...])\n",
    "# generator = tf.keras.models.load_model(\".\\models\\pressure_predict_tf2_Cv2.h5\")\n",
    "# generate_images(generator, X_train_rand[prev_idx][tf.newaxis, ...], Y_train_rand[prev_idx][tf.newaxis, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Export Model as ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "# Helper libraries for transposition of NHWC (model) to NCHW (Houdini)\n",
    "import onnx\n",
    "\n",
    "# Load model\n",
    "model = tf.keras.models.load_model(\".\\models\\pressure_predict_tf2_Cv4.h5\")\n",
    "\n",
    "output_path = \".\\models\\pressure_predict_tf2_Cv4-5.onnx\"\n",
    "\n",
    "# Define batch dim\n",
    "input_shape = (None, 64, 64, 3)  # Add None as batch dimension\n",
    "\n",
    "# Create input signature with batch dimension\n",
    "input_signature = [tf.TensorSpec(shape=input_shape, dtype=tf.float32, name='input')]\n",
    "\n",
    "# Convert the model\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=input_signature, inputs_as_nchw=['input'],  outputs_as_nchw=['conv2d_transpose_5'], opset=13)\n",
    "\n",
    "# Save the model\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(model_proto.SerializeToString())\n",
    "# tf2onnx.utils.save_onnx_model(onnx_model, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define batch dim\n",
    "input_shape = (None, 64, 64, 3)  # Add None as batch dimension\n",
    "\n",
    "# Create input signature with batch dimension\n",
    "input_signature = [tf.TensorSpec(shape=input_shape, dtype=tf.float32, name='input')]\n",
    "\n",
    "# Convert to ONNX\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    model,\n",
    "    input_signature=input_signature,\n",
    "    opset=13,\n",
    "    inputs_as_nchw=['input'],\n",
    "    #outputs_as_nchw=['output']  # As you mentioned this was the correct setting\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "input_signature = [tf.TensorSpec((None, 64, 64, 3), tf.float32, name='input')]\n",
    "\n",
    "# Create custom configuration to prevent automatic transpose\n",
    "extra_opset = []\n",
    "inputs_as_nchw = ['input']\n",
    "custom_ops = {}\n",
    "custom_op_handlers = {}\n",
    "custom_rewriter = {}\n",
    "\n",
    "# Set target correctly\n",
    "target = None  # Let tf2onnx use default target\n",
    "# Or specify explicitly if needed:\n",
    "# target = \"onnx\"\n",
    "\n",
    "onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "    model,\n",
    "    input_signature=input_signature,\n",
    "    opset=13,\n",
    "    inputs_as_nchw=inputs_as_nchw,\n",
    "    custom_ops=custom_ops,\n",
    "    custom_op_handlers=custom_op_handlers,\n",
    "    custom_rewriter=custom_rewriter,\n",
    "    target=target,\n",
    "    extra_opset=extra_opset\n",
    ")\n",
    "\n",
    "with open(output_path, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "\n",
    "print(f\"Model successfully converted to ONNX and saved as {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose saved model\n",
    "import onnx\n",
    "from onnx import helper\n",
    "\n",
    "def add_transpose_layers(model_path, output_path):\n",
    "    # Load the model\n",
    "    model = onnx.load(model_path)\n",
    "    \n",
    "    # Get the original input/output info\n",
    "    original_input = model.graph.input[0]\n",
    "    original_output = model.graph.output[0]\n",
    "    input_name = original_input.name\n",
    "    output_name = original_output.name\n",
    "    \n",
    "    # Store the name of the first node's input\n",
    "    first_node_input = None\n",
    "    for node in model.graph.node:\n",
    "        if input_name in node.input:\n",
    "            first_node_input = node.input[:]  # Make a copy of the inputs\n",
    "            # Replace the original input name with our transposed version\n",
    "            for i, inp in enumerate(node.input):\n",
    "                if inp == input_name:\n",
    "                    node.input[i] = input_name + \"_transposed\"\n",
    "            break\n",
    "    \n",
    "    if first_node_input is None:\n",
    "        raise ValueError(\"Could not find the first node that uses the input\")\n",
    "    \n",
    "    # Find the last node that produces the output\n",
    "    last_node_output = None\n",
    "    for node in reversed(model.graph.node):\n",
    "        if output_name in node.output:\n",
    "            last_node_output = node.output[:]  # Make a copy of the outputs\n",
    "            # Replace the original output name with our intermediate version\n",
    "            for i, out in enumerate(node.output):\n",
    "                if out == output_name:\n",
    "                    node.output[i] = output_name + \"_pretranspose\"\n",
    "            break\n",
    "    \n",
    "    if last_node_output is None:\n",
    "        raise ValueError(\"Could not find the last node that produces the output\")\n",
    "    \n",
    "    # Create input transpose node (NHWC -> NCHW)\n",
    "    input_transpose = helper.make_node(\n",
    "        'Transpose',\n",
    "        inputs=[input_name],\n",
    "        outputs=[input_name + \"_transposed\"],\n",
    "        perm=[0, 1, 3, 2]  # NHWC -> NCHW\n",
    "    )\n",
    "    \n",
    "    # Create output transpose node (NCHW -> NHWC)\n",
    "    output_transpose = helper.make_node(\n",
    "        'Transpose',\n",
    "        inputs=[output_name + \"_pretranspose\"],\n",
    "        outputs=[output_name],\n",
    "        perm=[0, 1, 3, 2]  # NCHW -> NHWC\n",
    "    )\n",
    "    \n",
    "    # Insert the transpose nodes\n",
    "    model.graph.node.insert(0, input_transpose)  # Add input transpose at the start\n",
    "    model.graph.node.append(output_transpose)    # Add output transpose at the end\n",
    "    \n",
    "    # Save the modified model\n",
    "    onnx.save(model, output_path)\n",
    "    \n",
    "    # Verify the model\n",
    "    onnx.checker.check_model(model)\n",
    "    \n",
    "    print(\"Model modified and verified successfully!\")\n",
    "\n",
    "\n",
    "onnx_model = \".\\models\\pressure_predict_tf2_Cv4-5.onnx\"\n",
    "onnx_model_transposed = \".\\models\\pressure_predict_tf2_Cv4-5_tp2.onnx\"\n",
    "\n",
    "add_transpose_layers(onnx_model, onnx_model_transposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hOwDANkMWU3w"
   ],
   "name": "pressure_predict_test_MEG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
