{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07df8c95-c88d-46fd-8c16-ca884e3b386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 17 02:12:37 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.72                 Driver Version: 566.14         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070        On  |   00000000:2B:00.0  On |                  N/A |\n",
      "| 30%   38C    P8             21W /  220W |     703MiB /   8192MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "/bin/bash: line 1: nvcc: command not found\n",
      "2024-11-17 02:12:37.733958: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-17 02:12:37.734011: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-17 02:12:37.734059: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 02:12:37.740537: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2.14.1\n",
      "2024-11-17 02:12:39.687322: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-17 02:12:39.687371: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-17 02:12:39.687424: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 02:12:39.693374: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-17 02:12:41.050818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:41.055700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:41.055769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "Num GPUs Available: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 02:12:41.655401: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-17 02:12:41.655457: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-17 02:12:41.655485: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-17 02:12:41.662348: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GPU Devices ===\n",
      "Found 1 GPU(s)\n",
      "Name: /physical_device:GPU:0, Type: GPU\n",
      "\n",
      "=== CUDA Configuration ===\n",
      "CUDA_VISIBLE_DEVICES: Not Set\n",
      "XLA_FLAGS: Not Set\n",
      "\n",
      "=== Memory Allocation Test ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 02:12:43.148541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:43.152764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:43.152803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:43.156737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:43.156781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:43.156803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:43.413205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:43.413293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:43.413305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-11-17 02:12:43.413339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-17 02:12:43.413357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully allocated GPU memory and performed computation\n",
      "\n",
      "=== Device Placement ===\n",
      "Checking where operations are being placed...\n",
      "Operation successfully ran on GPU\n",
      "=== Environment Variables ===\n",
      "CUDA_HOME: Not Set\n",
      "CUDA_PATH: Not Set\n",
      "PATH: ['/home/megger/miniconda3/envs/pressure_predict/bin:/home/megger/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/ProgramData/miniconda3/condabin:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/libnvvp:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64:/mnt/c/Windows/system32:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/Common Files/Intel/Shared Libraries/redist/intel64/compiler:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Program Files/Microsoft SQL Server/110/Tools/Binn:/mnt/c/Program Files/ffmpeg-4.2.2/bin:/mnt/c/Program Files (x86)/QuickTime/QTSystem:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files/NOVIDIA GPU Computing Toolkit/CUDA/v8.0/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/ProgramData/Anaconda3/condabin:/mnt/c/Program Files/CMake/bin:/mnt/c/Program Files/dotnet:/mnt/c/ProgramData/Anaconda3/include:/mnt/c/Program Files (x86)/GtkSharp/2.12/bin:/mnt/c/Program Files/Microsoft VS Code/bin:/mnt/c/Program Files/NVIDIA Corporation/Nsight Compute 2020.3.0:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/extras/CUPTI/lib64:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include:/Docker/host/bin:/mnt/c/Users/Martin Egger/AppData/Local/Programs/Python/Python37/Scripts:/mnt/c/Users/Martin Egger/AppData/Local/Programs/Python/Python37:/mnt/c/Users/Martin Egger/Desktop/timer:/mnt/c/Users/Martin Egger/AppData/Local/atom/bin:/mnt/c/Users/Martin Egger/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Martin Egger/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Martin Egger/AppData/Local/GitHubDesktop/bin:/mnt/c/Program Files/JetBrains/PyCharm Community Edition 2022.1/bin:/mnt/c/Users/Martin Egger/.dotnet/tools:/mnt/c/Users/Martin Egger/AppData/Roaming/Python/Python37/Scripts:/mnt/c/Users/Martin Egger/AppData/Roaming/Python/Python37:/mnt/c/Work/22-04_tempoGAN/mantaflow-master/build14_np/Release:/mnt/c/Work/22-04_multipassGAN/tools_wscale:/snap/bin']\n",
      "\n",
      "=== GPU Performance Test ===\n",
      "Time taken for 10 5000x5000 matrix multiplications: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1. System Information Gathering\n",
    "!nvidia-smi  # Check if GPU is recognized at system level\n",
    "!nvcc --version  # Check CUDA compiler version\n",
    "!python -c \"import tensorflow as tf; print(tf.__version__)\"  # TF version\n",
    "!python -c \"import tensorflow as tf; print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))\"\n",
    "\n",
    "# 2. Detailed GPU Configuration Check\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def check_gpu_configuration():\n",
    "    # Check if TF can see the GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(\"\\n=== GPU Devices ===\")\n",
    "    print(f\"Found {len(gpus)} GPU(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"Name: {gpu.name}, Type: {gpu.device_type}\")\n",
    "    \n",
    "    # Check if CUDA is properly linked\n",
    "    print(\"\\n=== CUDA Configuration ===\")\n",
    "    print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not Set')}\")\n",
    "    print(f\"XLA_FLAGS: {os.environ.get('XLA_FLAGS', 'Not Set')}\")\n",
    "    \n",
    "    # Test GPU memory allocation\n",
    "    print(\"\\n=== Memory Allocation Test ===\")\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.random.normal([1000, 1000])\n",
    "            b = tf.random.normal([1000, 1000])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(\"Successfully allocated GPU memory and performed computation\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU memory allocation failed: {str(e)}\")\n",
    "    \n",
    "    # Check if GPU is actually being used\n",
    "    print(\"\\n=== Device Placement ===\")\n",
    "    print(\"Checking where operations are being placed...\")\n",
    "    \n",
    "    @tf.function\n",
    "    def test_func():\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.random.normal([1000, 1000])\n",
    "            b = tf.random.normal([1000, 1000])\n",
    "            return tf.matmul(a, b)\n",
    "    \n",
    "    with tf.device('/GPU:0'):\n",
    "        result = test_func()\n",
    "        print(\"Operation successfully ran on GPU\")\n",
    "\n",
    "# Run all checks\n",
    "check_gpu_configuration()\n",
    "\n",
    "# 3. Environment Variable Check Command\n",
    "import os\n",
    "print(\"=== Environment Variables ===\")\n",
    "print(f\"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not Set')}\")\n",
    "print(f\"CUDA_PATH: {os.environ.get('CUDA_PATH', 'Not Set')}\")  # Windows often uses this instead\n",
    "print(f\"PATH: {[p for p in os.environ.get('PATH', '').split(';') if 'cuda' in p.lower()]}\")\n",
    "\n",
    "# 4. Test GPU Performance\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def test_gpu_performance():\n",
    "    # Create large tensors\n",
    "    size = 5000\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Warm-up\n",
    "        a = tf.random.normal([size, size])\n",
    "        b = tf.random.normal([size, size])\n",
    "        tf.matmul(a, b)\n",
    "        \n",
    "        # Actual test\n",
    "        start_time = time.time()\n",
    "        for _ in range(10):\n",
    "            tf.matmul(a, b)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\n=== GPU Performance Test ===\")\n",
    "        print(f\"Time taken for 10 {size}x{size} matrix multiplications: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "test_gpu_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de19933-803b-4637-8816-6b2b923a37e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TensorFlow Version: 2.14.1\n",
      "Python Version: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) \n",
      "[GCC 12.3.0]\n",
      "\n",
      "GPU Devices:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "CUDA Visible Devices: Not Set\n",
      "\n",
      "GPU Test Operation Result:\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "GPU test passed successfully!\n",
      "\n",
      "GPU Memory Details:\n",
      "{'current': 2560, 'peak': 400001792}\n",
      "\n",
      "Built with CUDA: True\n",
      "\n",
      "GPU Details: {'compute_capability': (8, 6), 'device_name': 'NVIDIA GeForce RTX 3070'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 02:13:00.214925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def check_gpu_setup():\n",
    "    \"\"\"\n",
    "    Comprehensive check of TensorFlow GPU setup and configuration.\n",
    "    Prints detailed information about the environment and GPU availability.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TensorFlow Version:\", tf.__version__)\n",
    "    print(\"Python Version:\", sys.version)\n",
    "    \n",
    "    # Check if GPU is visible to TensorFlow\n",
    "    print(\"\\nGPU Devices:\")\n",
    "    print(tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    print(\"\\nCUDA Visible Devices:\", os.environ.get('CUDA_VISIBLE_DEVICES', 'Not Set'))\n",
    "    \n",
    "    # Try to perform a simple GPU operation\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(\"\\nGPU Test Operation Result:\")\n",
    "            print(c)\n",
    "            print(\"\\nGPU test passed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"\\nGPU test failed with error:\")\n",
    "        print(str(e))\n",
    "    \n",
    "    # Print GPU memory info if available\n",
    "    try:\n",
    "        print(\"\\nGPU Memory Details:\")\n",
    "        print(tf.config.experimental.get_memory_info('GPU:0'))\n",
    "    except:\n",
    "        print(\"\\nUnable to get GPU memory details\")\n",
    "    \n",
    "    # Check if CUDA is built with TensorFlow\n",
    "    print(\"\\nBuilt with CUDA:\", tf.test.is_built_with_cuda())\n",
    "    \n",
    "    if hasattr(tf.config.experimental, 'get_device_details'):\n",
    "        for device in tf.config.list_physical_devices('GPU'):\n",
    "            details = tf.config.experimental.get_device_details(device)\n",
    "            print(\"\\nGPU Details:\", details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gpu_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84cf6d27-315d-4e8b-8fd3-cf6411839189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: ./logs/20241117-183429 (size: 2459 bytes)\n"
     ]
    }
   ],
   "source": [
    "# DELETE SMALL LOG DIRECTORIES\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_small_directories(root_dir: str, size_threshold: int):\n",
    "    \"\"\"\n",
    "    Traverse a directory and delete all subdirectories smaller than the given size.\n",
    "\n",
    "    Parameters:\n",
    "        root_dir (str): Path to the root directory.\n",
    "        size_threshold (int): Size threshold in bytes. Subdirectories smaller than this will be deleted.\n",
    "    \"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir, topdown=False):\n",
    "        for dirname in dirnames:\n",
    "            subdir_path = os.path.join(dirpath, dirname)\n",
    "            total_size = sum(\n",
    "                os.path.getsize(os.path.join(root, file))\n",
    "                for root, _, files in os.walk(subdir_path)\n",
    "                for file in files\n",
    "            )\n",
    "            if total_size < size_threshold:\n",
    "                shutil.rmtree(subdir_path)\n",
    "                print(f\"Deleted: {subdir_path} (size: {total_size} bytes)\")\n",
    "\n",
    "delete_small_directories(\"./logs\", 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183cc64-90ab-4f07-abf3-2e5e8041cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # def consistency_regularization(self, real_images, fake_images):\n",
    "    #     \"\"\"Additional regularization term for stability\"\"\"\n",
    "    #     # Add small random perturbation\n",
    "    #     epsilon = 1e-3\n",
    "    #     perturbed_real = real_images + tf.random.normal(tf.shape(real_images)) * epsilon\n",
    "    #     perturbed_fake = fake_images + tf.random.normal(tf.shape(fake_images)) * epsilon\n",
    "        \n",
    "    #     # Get discriminator outputs for perturbed images\n",
    "    #     d_real = self.discriminator([perturbed_real, real_images])\n",
    "    #     d_fake = self.discriminator([perturbed_fake, fake_images])\n",
    "        \n",
    "    #     # Compute consistency loss\n",
    "    #     consistency_loss = (\n",
    "    #         tf.reduce_mean(tf.square(d_real - self.discriminator([real_images, real_images]))) +\n",
    "    #         tf.reduce_mean(tf.square(d_fake - self.discriminator([fake_images, fake_images])))\n",
    "    #     )\n",
    "        \n",
    "    #     return consistency_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26d7e6-d5ee-45ca-a90f-f652462ae1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
