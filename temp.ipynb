{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07df8c95-c88d-46fd-8c16-ca884e3b386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid combination of input arguments. Please run 'nvidia-smi -h' for help.\n",
      "\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Nov_30_19:15:10_Pacific_Standard_Time_2020\n",
      "Cuda compilation tools, release 11.2, V11.2.67\n",
      "Build cuda_11.2.r11.2/compiler.29373293_0\n",
      "2.10.0\n",
      "Num GPUs Available: 1\n",
      "\n",
      "=== GPU Devices ===\n",
      "Found 1 GPU(s)\n",
      "Name: /physical_device:GPU:0, Type: GPU\n",
      "\n",
      "=== CUDA Configuration ===\n",
      "CUDA_VISIBLE_DEVICES: Not Set\n",
      "XLA_FLAGS: Not Set\n",
      "\n",
      "=== Memory Allocation Test ===\n",
      "Successfully allocated GPU memory and performed computation\n",
      "\n",
      "=== Device Placement ===\n",
      "Checking where operations are being placed...\n",
      "Operation successfully ran on GPU\n",
      "=== Environment Variables ===\n",
      "CUDA_HOME: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\n",
      "CUDA_PATH: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\n",
      "PATH: ['C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.2\\\\bin', 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.2\\\\libnvvp', 'C:\\\\Program Files\\\\NOVIDIA GPU Computing Toolkit\\\\CUDA\\\\v8.0\\\\bin']\n",
      "\n",
      "=== GPU Performance Test ===\n",
      "Time taken for 10 5000x5000 matrix multiplications: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1. System Information Gathering\n",
    "!nvidia-smi  # Check if GPU is recognized at system level\n",
    "!nvcc --version  # Check CUDA compiler version\n",
    "!python -c \"import tensorflow as tf; print(tf.__version__)\"  # TF version\n",
    "!python -c \"import tensorflow as tf; print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))\"\n",
    "\n",
    "# 2. Detailed GPU Configuration Check\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def check_gpu_configuration():\n",
    "    # Check if TF can see the GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(\"\\n=== GPU Devices ===\")\n",
    "    print(f\"Found {len(gpus)} GPU(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"Name: {gpu.name}, Type: {gpu.device_type}\")\n",
    "    \n",
    "    # Check if CUDA is properly linked\n",
    "    print(\"\\n=== CUDA Configuration ===\")\n",
    "    print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not Set')}\")\n",
    "    print(f\"XLA_FLAGS: {os.environ.get('XLA_FLAGS', 'Not Set')}\")\n",
    "    \n",
    "    # Test GPU memory allocation\n",
    "    print(\"\\n=== Memory Allocation Test ===\")\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.random.normal([1000, 1000])\n",
    "            b = tf.random.normal([1000, 1000])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(\"Successfully allocated GPU memory and performed computation\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU memory allocation failed: {str(e)}\")\n",
    "    \n",
    "    # Check if GPU is actually being used\n",
    "    print(\"\\n=== Device Placement ===\")\n",
    "    print(\"Checking where operations are being placed...\")\n",
    "    \n",
    "    @tf.function\n",
    "    def test_func():\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.random.normal([1000, 1000])\n",
    "            b = tf.random.normal([1000, 1000])\n",
    "            return tf.matmul(a, b)\n",
    "    \n",
    "    with tf.device('/GPU:0'):\n",
    "        result = test_func()\n",
    "        print(\"Operation successfully ran on GPU\")\n",
    "\n",
    "# Run all checks\n",
    "check_gpu_configuration()\n",
    "\n",
    "# 3. Environment Variable Check Command\n",
    "import os\n",
    "print(\"=== Environment Variables ===\")\n",
    "print(f\"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not Set')}\")\n",
    "print(f\"CUDA_PATH: {os.environ.get('CUDA_PATH', 'Not Set')}\")  # Windows often uses this instead\n",
    "print(f\"PATH: {[p for p in os.environ.get('PATH', '').split(';') if 'cuda' in p.lower()]}\")\n",
    "\n",
    "# 4. Test GPU Performance\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def test_gpu_performance():\n",
    "    # Create large tensors\n",
    "    size = 5000\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Warm-up\n",
    "        a = tf.random.normal([size, size])\n",
    "        b = tf.random.normal([size, size])\n",
    "        tf.matmul(a, b)\n",
    "        \n",
    "        # Actual test\n",
    "        start_time = time.time()\n",
    "        for _ in range(10):\n",
    "            tf.matmul(a, b)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\n=== GPU Performance Test ===\")\n",
    "        print(f\"Time taken for 10 {size}x{size} matrix multiplications: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "test_gpu_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de19933-803b-4637-8816-6b2b923a37e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TensorFlow Version: 2.10.0\n",
      "Python Version: 3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:39:05) [MSC v.1929 64 bit (AMD64)]\n",
      "\n",
      "GPU Devices:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "CUDA Visible Devices: Not Set\n",
      "\n",
      "GPU Test Operation Result:\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "GPU test passed successfully!\n",
      "\n",
      "GPU Memory Details:\n",
      "{'current': 2560, 'peak': 400001792}\n",
      "\n",
      "Built with CUDA: True\n",
      "\n",
      "GPU Details: {'device_name': 'GeForce RTX 3070', 'compute_capability': (8, 6)}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def check_gpu_setup():\n",
    "    \"\"\"\n",
    "    Comprehensive check of TensorFlow GPU setup and configuration.\n",
    "    Prints detailed information about the environment and GPU availability.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TensorFlow Version:\", tf.__version__)\n",
    "    print(\"Python Version:\", sys.version)\n",
    "    \n",
    "    # Check if GPU is visible to TensorFlow\n",
    "    print(\"\\nGPU Devices:\")\n",
    "    print(tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    print(\"\\nCUDA Visible Devices:\", os.environ.get('CUDA_VISIBLE_DEVICES', 'Not Set'))\n",
    "    \n",
    "    # Try to perform a simple GPU operation\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(\"\\nGPU Test Operation Result:\")\n",
    "            print(c)\n",
    "            print(\"\\nGPU test passed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"\\nGPU test failed with error:\")\n",
    "        print(str(e))\n",
    "    \n",
    "    # Print GPU memory info if available\n",
    "    try:\n",
    "        print(\"\\nGPU Memory Details:\")\n",
    "        print(tf.config.experimental.get_memory_info('GPU:0'))\n",
    "    except:\n",
    "        print(\"\\nUnable to get GPU memory details\")\n",
    "    \n",
    "    # Check if CUDA is built with TensorFlow\n",
    "    print(\"\\nBuilt with CUDA:\", tf.test.is_built_with_cuda())\n",
    "    \n",
    "    if hasattr(tf.config.experimental, 'get_device_details'):\n",
    "        for device in tf.config.list_physical_devices('GPU'):\n",
    "            details = tf.config.experimental.get_device_details(device)\n",
    "            print(\"\\nGPU Details:\", details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gpu_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf6d27-315d-4e8b-8fd3-cf6411839189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183cc64-90ab-4f07-abf3-2e5e8041cc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26d7e6-d5ee-45ca-a90f-f652462ae1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
