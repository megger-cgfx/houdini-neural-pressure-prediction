{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07df8c95-c88d-46fd-8c16-ca884e3b386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  5 12:13:40 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.72                 Driver Version: 566.14         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070        On  |   00000000:2B:00.0  On |                  N/A |\n",
      "| 30%   61C    P2             94W /  132W |    7906MiB /   8192MiB |     13%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A        39      G   /Xwayland                                   N/A      |\n",
      "|    0   N/A  N/A      3016      C   /python3.9                                  N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "/bin/bash: line 1: nvcc: command not found\n",
      "2024-12-05 12:13:40.717030: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-05 12:13:40.717099: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-05 12:13:40.717127: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2.14.1\n",
      "2024-12-05 12:13:42.706805: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-05 12:13:42.706864: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-05 12:13:42.706919: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Num GPUs Available: 1\n",
      "\n",
      "=== GPU Devices ===\n",
      "Found 1 GPU(s)\n",
      "Name: /physical_device:GPU:0, Type: GPU\n",
      "\n",
      "=== CUDA Configuration ===\n",
      "CUDA_VISIBLE_DEVICES: Not Set\n",
      "XLA_FLAGS: Not Set\n",
      "\n",
      "=== Memory Allocation Test ===\n",
      "Successfully allocated GPU memory and performed computation\n",
      "\n",
      "=== Device Placement ===\n",
      "Checking where operations are being placed...\n",
      "Operation successfully ran on GPU\n",
      "=== Environment Variables ===\n",
      "CUDA_HOME: Not Set\n",
      "CUDA_PATH: Not Set\n",
      "PATH: ['/home/megger/miniconda3/envs/pressure_predict/bin:/home/megger/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/ProgramData/miniconda3/condabin:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/bin:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/libnvvp:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/lib/x64:/mnt/c/Windows/system32:/mnt/c/Program Files/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/Program Files (x86)/Common Files/Intel/Shared Libraries/redist/intel64/compiler:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/Program Files/NVIDIA Corporation/NVIDIA NvDLISR:/mnt/c/Program Files/Microsoft SQL Server/110/Tools/Binn:/mnt/c/Program Files/ffmpeg-4.2.2/bin:/mnt/c/Program Files (x86)/QuickTime/QTSystem:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0:/mnt/c/WINDOWS/System32/OpenSSH:/mnt/c/Program Files/NOVIDIA GPU Computing Toolkit/CUDA/v8.0/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/ProgramData/Anaconda3/condabin:/mnt/c/Program Files/CMake/bin:/mnt/c/Program Files/dotnet:/mnt/c/ProgramData/Anaconda3/include:/mnt/c/Program Files (x86)/GtkSharp/2.12/bin:/mnt/c/Program Files/Microsoft VS Code/bin:/mnt/c/Program Files/NVIDIA Corporation/Nsight Compute 2020.3.0:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/extras/CUPTI/lib64:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/include:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/Program Files/Android/platform-tools:/mnt/c/Program Files/Android/cmdline-tools/bin:/mnt/c/Program Files/Android/cmdline-tools:/mnt/c/Program Files/SSHFS-Win/bin:/mnt/c/Users/Martin Egger/AppData/Local/Programs/Python/Python37/Scripts:/mnt/c/Users/Martin Egger/AppData/Local/Programs/Python/Python37:/mnt/c/Users/Martin Egger/Desktop/timer:/mnt/c/Users/Martin Egger/AppData/Local/atom/bin:/mnt/c/Users/Martin Egger/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Martin Egger/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/Martin Egger/AppData/Local/GitHubDesktop/bin:/mnt/c/Program Files/JetBrains/PyCharm Community Edition 2022.1/bin:/mnt/c/Users/Martin Egger/.dotnet/tools:/mnt/c/Users/Martin Egger/AppData/Roaming/Python/Python37/Scripts:/mnt/c/Users/Martin Egger/AppData/Roaming/Python/Python37:/mnt/c/Work/22-04_tempoGAN/mantaflow-master/build14_np/Release:/mnt/c/Work/22-04_multipassGAN/tools_wscale']\n",
      "\n",
      "=== GPU Performance Test ===\n",
      "Time taken for 10 5000x5000 matrix multiplications: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1. System Information Gathering\n",
    "!nvidia-smi  # Check if GPU is recognized at system level\n",
    "!nvcc --version  # Check CUDA compiler version\n",
    "!python -c \"import tensorflow as tf; print(tf.__version__)\"  # TF version\n",
    "!python -c \"import tensorflow as tf; print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))\"\n",
    "\n",
    "# 2. Detailed GPU Configuration Check\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def check_gpu_configuration():\n",
    "    # Check if TF can see the GPU\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    print(\"\\n=== GPU Devices ===\")\n",
    "    print(f\"Found {len(gpus)} GPU(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"Name: {gpu.name}, Type: {gpu.device_type}\")\n",
    "    \n",
    "    # Check if CUDA is properly linked\n",
    "    print(\"\\n=== CUDA Configuration ===\")\n",
    "    print(f\"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not Set')}\")\n",
    "    print(f\"XLA_FLAGS: {os.environ.get('XLA_FLAGS', 'Not Set')}\")\n",
    "    \n",
    "    # Test GPU memory allocation\n",
    "    print(\"\\n=== Memory Allocation Test ===\")\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.random.normal([1000, 1000])\n",
    "            b = tf.random.normal([1000, 1000])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(\"Successfully allocated GPU memory and performed computation\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU memory allocation failed: {str(e)}\")\n",
    "    \n",
    "    # Check if GPU is actually being used\n",
    "    print(\"\\n=== Device Placement ===\")\n",
    "    print(\"Checking where operations are being placed...\")\n",
    "    \n",
    "    @tf.function\n",
    "    def test_func():\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.random.normal([1000, 1000])\n",
    "            b = tf.random.normal([1000, 1000])\n",
    "            return tf.matmul(a, b)\n",
    "    \n",
    "    with tf.device('/GPU:0'):\n",
    "        result = test_func()\n",
    "        print(\"Operation successfully ran on GPU\")\n",
    "\n",
    "# Run all checks\n",
    "check_gpu_configuration()\n",
    "\n",
    "# 3. Environment Variable Check Command\n",
    "import os\n",
    "print(\"=== Environment Variables ===\")\n",
    "print(f\"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not Set')}\")\n",
    "print(f\"CUDA_PATH: {os.environ.get('CUDA_PATH', 'Not Set')}\")  # Windows often uses this instead\n",
    "print(f\"PATH: {[p for p in os.environ.get('PATH', '').split(';') if 'cuda' in p.lower()]}\")\n",
    "\n",
    "# 4. Test GPU Performance\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def test_gpu_performance():\n",
    "    # Create large tensors\n",
    "    size = 5000\n",
    "    with tf.device('/GPU:0'):\n",
    "        # Warm-up\n",
    "        a = tf.random.normal([size, size])\n",
    "        b = tf.random.normal([size, size])\n",
    "        tf.matmul(a, b)\n",
    "        \n",
    "        # Actual test\n",
    "        start_time = time.time()\n",
    "        for _ in range(10):\n",
    "            tf.matmul(a, b)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\n=== GPU Performance Test ===\")\n",
    "        print(f\"Time taken for 10 {size}x{size} matrix multiplications: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "test_gpu_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de19933-803b-4637-8816-6b2b923a37e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TensorFlow Version: 2.14.1\n",
      "Python Version: 3.9.19 | packaged by conda-forge | (main, Mar 20 2024, 12:50:21) \n",
      "[GCC 12.3.0]\n",
      "\n",
      "GPU Devices:\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "CUDA Visible Devices: Not Set\n",
      "\n",
      "GPU Test Operation Result:\n",
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n",
      "\n",
      "GPU test passed successfully!\n",
      "\n",
      "GPU Memory Details:\n",
      "{'current': 2560, 'peak': 400001792}\n",
      "\n",
      "Built with CUDA: True\n",
      "\n",
      "GPU Details: {'compute_capability': (8, 6), 'device_name': 'NVIDIA GeForce RTX 3070'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 02:13:00.214925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def check_gpu_setup():\n",
    "    \"\"\"\n",
    "    Comprehensive check of TensorFlow GPU setup and configuration.\n",
    "    Prints detailed information about the environment and GPU availability.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"TensorFlow Version:\", tf.__version__)\n",
    "    print(\"Python Version:\", sys.version)\n",
    "    \n",
    "    # Check if GPU is visible to TensorFlow\n",
    "    print(\"\\nGPU Devices:\")\n",
    "    print(tf.config.list_physical_devices('GPU'))\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    print(\"\\nCUDA Visible Devices:\", os.environ.get('CUDA_VISIBLE_DEVICES', 'Not Set'))\n",
    "    \n",
    "    # Try to perform a simple GPU operation\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "            c = tf.matmul(a, b)\n",
    "            print(\"\\nGPU Test Operation Result:\")\n",
    "            print(c)\n",
    "            print(\"\\nGPU test passed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(\"\\nGPU test failed with error:\")\n",
    "        print(str(e))\n",
    "    \n",
    "    # Print GPU memory info if available\n",
    "    try:\n",
    "        print(\"\\nGPU Memory Details:\")\n",
    "        print(tf.config.experimental.get_memory_info('GPU:0'))\n",
    "    except:\n",
    "        print(\"\\nUnable to get GPU memory details\")\n",
    "    \n",
    "    # Check if CUDA is built with TensorFlow\n",
    "    print(\"\\nBuilt with CUDA:\", tf.test.is_built_with_cuda())\n",
    "    \n",
    "    if hasattr(tf.config.experimental, 'get_device_details'):\n",
    "        for device in tf.config.list_physical_devices('GPU'):\n",
    "            details = tf.config.experimental.get_device_details(device)\n",
    "            print(\"\\nGPU Details:\", details)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_gpu_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84cf6d27-315d-4e8b-8fd3-cf6411839189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: ./logs/20241118-162136 (size: 78 bytes)\n",
      "Deleted: ./logs/20241118-162202 (size: 78 bytes)\n"
     ]
    }
   ],
   "source": [
    "# DELETE SMALL LOG DIRECTORIES\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def delete_small_directories(root_dir: str, size_threshold: int):\n",
    "    \"\"\"\n",
    "    Traverse a directory and delete all subdirectories smaller than the given size.\n",
    "\n",
    "    Parameters:\n",
    "        root_dir (str): Path to the root directory.\n",
    "        size_threshold (int): Size threshold in bytes. Subdirectories smaller than this will be deleted.\n",
    "    \"\"\"\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir, topdown=False):\n",
    "        for dirname in dirnames:\n",
    "            subdir_path = os.path.join(dirpath, dirname)\n",
    "            total_size = sum(\n",
    "                os.path.getsize(os.path.join(root, file))\n",
    "                for root, _, files in os.walk(subdir_path)\n",
    "                for file in files\n",
    "            )\n",
    "            if total_size < size_threshold:\n",
    "                shutil.rmtree(subdir_path)\n",
    "                print(f\"Deleted: {subdir_path} (size: {total_size} bytes)\")\n",
    "\n",
    "delete_small_directories(\"./logs\", 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0183cc64-90ab-4f07-abf3-2e5e8041cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # def consistency_regularization(self, real_images, fake_images):\n",
    "    #     \"\"\"Additional regularization term for stability\"\"\"\n",
    "    #     # Add small random perturbation\n",
    "    #     epsilon = 1e-3\n",
    "    #     perturbed_real = real_images + tf.random.normal(tf.shape(real_images)) * epsilon\n",
    "    #     perturbed_fake = fake_images + tf.random.normal(tf.shape(fake_images)) * epsilon\n",
    "        \n",
    "    #     # Get discriminator outputs for perturbed images\n",
    "    #     d_real = self.discriminator([perturbed_real, real_images])\n",
    "    #     d_fake = self.discriminator([perturbed_fake, fake_images])\n",
    "        \n",
    "    #     # Compute consistency loss\n",
    "    #     consistency_loss = (\n",
    "    #         tf.reduce_mean(tf.square(d_real - self.discriminator([real_images, real_images]))) +\n",
    "    #         tf.reduce_mean(tf.square(d_fake - self.discriminator([fake_images, fake_images])))\n",
    "    #     )\n",
    "        \n",
    "    #     return consistency_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca26d7e6-d5ee-45ca-a90f-f652462ae1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:13:26.674414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8700\n",
      "2024-12-05 12:13:26.998877: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-12-05 12:13:27.382366: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 3, 64, 64)\n",
      "Output shape: (1, 3, 64, 64)\n",
      "\n",
      "ONNX Model Input/Output Shapes:\n",
      "Input: input, Shape: [None, 3, None, None]\n",
      "Output: batch_normalization_1, Shape: [None, 3, None, None]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 12:13:27.612493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.612551: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2024-12-05 12:13:27.612718: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-12-05 12:13:27.613153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.613201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.613261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.613467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.613481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-12-05 12:13:27.613512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.613525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n",
      "2024-12-05 12:13:27.647593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.647676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.647704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.647995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.648040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-12-05 12:13:27.648080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.648098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n",
      "2024-12-05 12:13:27.653524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.653559: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2024-12-05 12:13:27.653729: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2024-12-05 12:13:27.654114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.654160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.654183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.654384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.654397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1977] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-12-05 12:13:27.654426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:880] could not open file to read NUMA node: /sys/bus/pci/devices/0000:2b:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-05 12:13:27.654439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "import onnx\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Create a simple model that maintains input dimensions\n",
    "def create_mock_model():\n",
    "    # Input layer expecting NCHW format\n",
    "    inputs = tf.keras.Input(shape=(3, None, None), name='input')\n",
    "    \n",
    "    # Simple convolution layers that maintain dimensions\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=3,\n",
    "        kernel_size=3,\n",
    "        padding='same',\n",
    "        data_format='channels_first'\n",
    "    )(inputs)\n",
    "    \n",
    "    x = tf.keras.layers.BatchNormalization(axis=1)(x)  # axis=1 for channels_first\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=3,\n",
    "        kernel_size=3,\n",
    "        padding='same',\n",
    "        data_format='channels_first'\n",
    "    )(x)\n",
    "    \n",
    "    # Ensure output has same dimensions as input\n",
    "    outputs = tf.keras.layers.BatchNormalization(axis=1)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "def export_to_onnx(model, save_path):\n",
    "    # Convert the model to ONNX\n",
    "    input_signature = [tf.TensorSpec([None, 3, None, None], tf.float32, name='input')]\n",
    "    onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "        model, \n",
    "        input_signature=input_signature,\n",
    "        opset=13,\n",
    "        output_path=save_path\n",
    "    )\n",
    "    return onnx_model\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_mock_model()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "# Test the model with sample data\n",
    "sample_data = tf.random.normal((1, 3, 64, 64))\n",
    "test_output = model(sample_data)\n",
    "print(f\"Input shape: {sample_data.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "\n",
    "# Export the model\n",
    "onnx_path = \"mock_model.onnx\"\n",
    "onnx_model = export_to_onnx(model, onnx_path)\n",
    "\n",
    "# Verify the ONNX model\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "print(\"\\nONNX Model Input/Output Shapes:\")\n",
    "for input in onnx_model.graph.input:\n",
    "    print(f\"Input: {input.name}, Shape: {[dim.dim_value if dim.dim_value != 0 else None for dim in input.type.tensor_type.shape.dim]}\")\n",
    "for output in onnx_model.graph.output:\n",
    "    print(f\"Output: {output.name}, Shape: {[dim.dim_value if dim.dim_value != 0 else None for dim in output.type.tensor_type.shape.dim]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32ac24-e216-4d74-8388-6508a3d7d149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
